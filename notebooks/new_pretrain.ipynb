{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import hashlib\n",
    "import base64\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "import simpy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from cycler import cycler\n",
    "from functools import partial\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "os.chdir(\"../src\")\n",
    "from dqnroute import *\n",
    "from dqnroute.networks import *\n",
    "os.chdir(current_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_in_simulation(run_params, router_type, pkg_num, training_router_type='link_state',\n",
    "                        breaks_num=0, max_breaks=2, delta=20):\n",
    "    period_len = pkg_num * delta\n",
    "    max_pause = delta * max_breaks\n",
    "    all_br_num = sum([breaks_num*cb for cb in range(1, max_breaks+1)])\n",
    "    approx_total_time = period_len + (period_len + max_pause) * all_br_num\n",
    "    \n",
    "    with tqdm(total=approx_total_time) as bar:\n",
    "        queue = DummyProgressbarQueue(bar)\n",
    "        brain, loss_df, launch_df = run_training(\n",
    "            NetworkRunner, run_params=run_params,\n",
    "            router_type=router_type, training_router_type=training_router_type,\n",
    "            pkg_num=5000, random_seed=42,\n",
    "            progress_queue=queue, progress_step=20)\n",
    "    return brain, loss_df, launch_df\n",
    "\n",
    "def gen_episodes_progress(num_episodes, **kwargs):\n",
    "    with tqdm(total=num_episodes) as bar:\n",
    "        df = gen_episodes(router_type='dqn_oneout', bar=bar, num_episodes=num_episodes, **kwargs)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(df):\n",
    "    return df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "def find_first_sublist(seq, sublist, start=0):\n",
    "    length = len(sublist)\n",
    "    for index in range(start, len(seq)):\n",
    "        if seq[index:index+length] == sublist:\n",
    "            return index, index+length\n",
    "\n",
    "def replace_sublist(seq, sublist, replacement):\n",
    "    length = len(replacement)\n",
    "    index = 0\n",
    "    for start, end in iter(lambda: find_first_sublist(seq, sublist, index), None):\n",
    "        seq[start:end] = replacement\n",
    "        index = start + length\n",
    "    return seq\n",
    "\n",
    "def transform_to_one_out(df):\n",
    "    old_cols = list(df.columns)\n",
    "    neighbors_cols = [col for col in old_cols if col.startswith('neighbors')]\n",
    "    target_cols = [col for col in old_cols if col.startswith('predict')]\n",
    "    \n",
    "    new_cols = replace_sublist(replace_sublist(old_cols, neighbors_cols, ['neighbour']),\n",
    "                               target_cols, ['predict'])\n",
    "    row_ix = 0\n",
    "    nums = pd.Series(range(len(neighbors_cols)), index=neighbors_cols)\n",
    "    new_rows_num = df[neighbors_cols].sum().sum()\n",
    "    df_new = pd.DataFrame(columns=new_cols, index=np.arange(new_rows_num), dtype=np.float32)\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        nbrs = nums[row[neighbors_cols] != 0]\n",
    "        preds = list(row[target_cols][row != -1000000])        \n",
    "        new_row_tpl = row.drop(neighbors_cols + target_cols)\n",
    "        \n",
    "        for nbr, pred in zip(nbrs, preds):\n",
    "            new_row = new_row_tpl\n",
    "            new_row['neighbour'] = nbr\n",
    "            new_row['predict'] = pred\n",
    "            df_new.loc[row_ix] = new_row\n",
    "            row_ix += 1\n",
    "        \n",
    "    return df_new.reindex(np.arange(len(df_new)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_graph(graph):\n",
    "    if type(graph) != np.ndarray:\n",
    "        graph = nx.to_numpy_matrix(graph, nodelist=sorted(graph.nodes))\n",
    "    m = hashlib.sha256()\n",
    "    m.update(graph.tobytes())\n",
    "    return base64.b64encode(m.digest()).decode('utf-8')\n",
    "\n",
    "class CachedEmbedding(Embedding):\n",
    "    def __init__(self, InnerEmbedding, dim, **kwargs):\n",
    "        self.dim = dim\n",
    "        self.InnerEmbedding = InnerEmbedding\n",
    "        self.inner_kwargs = kwargs\n",
    "        self.fit_embeddings = {}\n",
    "        \n",
    "    def fit(self, graph, **kwargs):\n",
    "        h = hash_graph(graph)\n",
    "        if h not in self.fit_embeddings:\n",
    "            embed = self.InnerEmbedding(dim=self.dim, **self.inner_kwargs)\n",
    "            embed.fit(graph, **kwargs)\n",
    "            self.fit_embeddings[h] = embed\n",
    "    \n",
    "    def transform(self, graph, idx):\n",
    "        h = hash_graph(graph)\n",
    "        return self.fit_embeddings[h].transform(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_or_emb(vals, embedding=None):\n",
    "    if embedding is None:\n",
    "        return vals\n",
    "    return embedding.get_embedding(vals.astype(int))\n",
    "\n",
    "def add_inp_cols(tag, dim):\n",
    "    return mk_num_list(tag + '_', dim) if dim > 1 else tag\n",
    "\n",
    "def qnetwork_batches(net, data, batch_size=64, embedding=None):\n",
    "    n = net.graph_size\n",
    "    data_cols = []\n",
    "    amatrix_cols = get_amatrix_cols(n)\n",
    "    \n",
    "    for (tag, dim) in net.add_inputs:\n",
    "        if tag == 'amatrix':\n",
    "            data_cols.append(amatrix_cols)\n",
    "        else:\n",
    "            data_cols.append(add_inp_cols(tag, dim))\n",
    "\n",
    "    for (a, b) in make_batches(data.shape[0], batch_size):\n",
    "        batch = data[a:b]\n",
    "        addr = batch['addr'].values\n",
    "        dst = batch['dst'].values\n",
    "        nbr = batch['neighbour'].values\n",
    "        \n",
    "        if embedding is not None:\n",
    "            amatrices = batch[amatrix_cols].values\n",
    "            new_btch = []\n",
    "            for (addr_, dst_, nbr_, A) in zip(addr, dst, nbr, amatrices):\n",
    "                A = A.reshape(n, n)\n",
    "                embedding.fit(A)\n",
    "                new_addr = embedding.transform(A, int(addr_))\n",
    "                new_dst = embedding.transform(A, int(dst_))\n",
    "                new_nbr = embedding.transform(A, int(nbr_))\n",
    "                new_btch.append((new_addr, new_dst, new_nbr))\n",
    "                \n",
    "            [addr, dst, nbr] = stack_batch(new_btch)\n",
    "            \n",
    "        addr_inp = torch.tensor(addr, dtype=torch.float)\n",
    "        dst_inp = torch.tensor(dst, dtype=torch.float)\n",
    "        nbr_inp = torch.tensor(nbr, dtype=torch.float)\n",
    "                \n",
    "        inputs = tuple(torch.tensor(batch[cols].values, dtype=torch.float)\n",
    "                       for cols in data_cols)\n",
    "        output = torch.tensor(batch['predict'].values, dtype=torch.float)\n",
    "        \n",
    "        yield ((addr_inp, dst_inp, nbr_inp) + inputs, output)\n",
    "\n",
    "def qnetwork_pretrain_epoch(net, optimizer, data, **kwargs):\n",
    "    loss_func = nn.MSELoss()\n",
    "    for (batch, target) in qnetwork_batches(net, data, **kwargs):\n",
    "        optimizer.zero_grad()\n",
    "        output = net(*batch)\n",
    "        loss = loss_func(output, target.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        yield float(loss)\n",
    "        \n",
    "def qnetwork_pretrain(net, data, optimizer='rmsprop', epochs=1, save_net=True, **kwargs):\n",
    "    optimizer = get_optimizer(optimizer)(net.parameters())\n",
    "    epochs_losses = []\n",
    "    \n",
    "    for i in tqdm(range(epochs)):\n",
    "        sum_loss = 0\n",
    "        loss_cnt = 0\n",
    "        for loss in tqdm(qnetwork_pretrain_epoch(net, optimizer, data, **kwargs),\n",
    "                         desc='epoch {}'.format(i)):\n",
    "            sum_loss += loss\n",
    "            loss_cnt += 1\n",
    "        epochs_losses.append(sum_loss / loss_cnt)\n",
    "        \n",
    "    if save_net:\n",
    "        net.save()\n",
    "    \n",
    "    return epochs_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_losses(losses_dict, path):\n",
    "    df = pd.DataFrame(losses_dict)\n",
    "    df.to_csv(path, index=False)\n",
    "\n",
    "def load_losses(path):\n",
    "    df = pd.read_csv(path, index_col=False)\n",
    "    return df.to_dict(orient='list')\n",
    "\n",
    "def plot_losses(losses_dict, from_epoch=0, num_epochs=None, xlim=None, ylim=None,\n",
    "                fsize=16, figsize=(13, 3), title=None, save_path=None):\n",
    "    if num_epochs is None:\n",
    "        num_epochs = len(next(iter(losses_dict.values())))\n",
    "        \n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    \n",
    "    x = range(from_epoch+1, num_epochs+1)\n",
    "    for (label, losses) in losses_dict.items():\n",
    "        plt.plot(x, losses[from_epoch:num_epochs], label=label)\n",
    "    plt.legend(prop={'size': fsize})\n",
    "    plt.xlabel('Epoch', fontsize=fsize)\n",
    "    plt.xticks(x)\n",
    "    plt.grid()\n",
    "    plt.ylabel('MSE', fontsize=fsize)\n",
    "    \n",
    "    if xlim is not None:\n",
    "        plt.xlim(xlim)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(ylim)\n",
    "    if title is not None:\n",
    "        plt.title(title, fontsize=fsize)\n",
    "        \n",
    "    plt.show(fig)\n",
    "    if save_path is not None:\n",
    "        fig.savefig('../img/' + save_path, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/10000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "64b9c63a766842c4836237e78f0aa536"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'network'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Input \u001B[1;32mIn [26]\u001B[0m, in \u001B[0;36m<cell line: 8>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      5\u001B[0m scenario, emb_dim, graph_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../launches/conveyor_topology_tarau/tarau2010_graph_original.yaml\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;241m8\u001B[39m, \u001B[38;5;241m27\u001B[39m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m#scenario, emb_dim, graph_sizeph_size = '../launches/igor/johnstone2010.yaml', 12, 54\u001B[39;00m\n\u001B[1;32m----> 8\u001B[0m data_conv \u001B[38;5;241m=\u001B[39m \u001B[43mgen_episodes_progress\u001B[49m\u001B[43m(\u001B[49m\u001B[43mignore_saved\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcontext\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mnetwork\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_episodes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_seed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m42\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrun_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mscenario\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msave_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m../logs/data_conveyor1_oneinp_new.csv\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [14]\u001B[0m, in \u001B[0;36mgen_episodes_progress\u001B[1;34m(num_episodes, **kwargs)\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgen_episodes_progress\u001B[39m(num_episodes, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m     18\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m tqdm(total\u001B[38;5;241m=\u001B[39mnum_episodes) \u001B[38;5;28;01mas\u001B[39;00m bar:\n\u001B[1;32m---> 19\u001B[0m         df \u001B[38;5;241m=\u001B[39m \u001B[43mgen_episodes\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrouter_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdqn_oneout\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbar\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_episodes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_episodes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     20\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m df\n",
      "File \u001B[1;32m~\\University\\Thesis\\dqnroute\\src\\dqnroute\\generator.py:221\u001B[0m, in \u001B[0;36mgen_episodes\u001B[1;34m(router_type, num_episodes, context, one_out, sinks, bar, random_seed, router_params, save_path, ignore_saved, run_params, use_full_topology)\u001B[0m\n\u001B[0;32m    216\u001B[0m router_params[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrandom_init\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    217\u001B[0m params_override \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msettings\u001B[39m\u001B[38;5;124m'\u001B[39m: {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrouter\u001B[39m\u001B[38;5;124m'\u001B[39m: {router_type: {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrandom_init\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;28;01mTrue\u001B[39;00m}}}\n\u001B[0;32m    219\u001B[0m }\n\u001B[1;32m--> 221\u001B[0m runner \u001B[38;5;241m=\u001B[39m \u001B[43mRunnerClass\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrouter_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrouter_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams_override\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams_override\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    222\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m runner\u001B[38;5;241m.\u001B[39mcontext \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnetwork\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m    223\u001B[0m     factory \u001B[38;5;241m=\u001B[39m runner\u001B[38;5;241m.\u001B[39mworld\u001B[38;5;241m.\u001B[39mfactory\n",
      "File \u001B[1;32m~\\University\\Thesis\\dqnroute\\src\\dqnroute\\simulation\\network.py:166\u001B[0m, in \u001B[0;36mNetworkRunner.__init__\u001B[1;34m(self, data_dir, **kwargs)\u001B[0m\n\u001B[0;32m    165\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, data_dir\u001B[38;5;241m=\u001B[39mLOG_DATA_DIR\u001B[38;5;241m+\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/network\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m--> 166\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdata_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\University\\Thesis\\dqnroute\\src\\dqnroute\\simulation\\common.py:230\u001B[0m, in \u001B[0;36mSimulationRunner.__init__\u001B[1;34m(self, run_params, data_dir, params_override, data_series, series_period, series_funcs, **kwargs)\u001B[0m\n\u001B[0;32m    228\u001B[0m \u001B[38;5;66;03m# Makes a world simulation\u001B[39;00m\n\u001B[0;32m    229\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv \u001B[38;5;241m=\u001B[39m Environment()\n\u001B[1;32m--> 230\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mworld \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmakeMultiAgentEnv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\University\\Thesis\\dqnroute\\src\\dqnroute\\simulation\\network.py:173\u001B[0m, in \u001B[0;36mNetworkRunner.makeMultiAgentEnv\u001B[1;34m(self, **kwargs)\u001B[0m\n\u001B[0;32m    171\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmakeMultiAgentEnv\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m MultiAgentEnv:\n\u001B[0;32m    172\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m NetworkEnvironment(env\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv, data_series\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_series,\n\u001B[1;32m--> 173\u001B[0m                               network_cfg\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_params\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mnetwork\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m,\n\u001B[0;32m    174\u001B[0m                               routers_cfg\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrun_params[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msettings\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrouter\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m    175\u001B[0m                               \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrun_params[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msettings\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrouter_env\u001B[39m\u001B[38;5;124m'\u001B[39m], \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "\u001B[1;31mKeyError\u001B[0m: 'network'"
     ]
    }
   ],
   "source": [
    "#scenario, emd_dim, graph_size = '../launches/igor/acyclic_conveyor_energy_test.yaml', 10, 20\n",
    "# scenario, emb_dim, graph_size = '../launches/igor/conveyor_cyclic_energy_test.yaml', 5, 8\n",
    "#scenario, emb_dim, graph_size = '../launches/igor/conveyor_cyclic2_energy_test.yaml', 6, 16\n",
    "#scenario, emb_dim, graph_size = '../launches/igor/tarau2010.yaml', 8, 27\n",
    "scenario, emb_dim, graph_size = '../launches/conveyor_topology_tarau/tarau2010_graph_original.yaml', 8, 27\n",
    "#scenario, emb_dim, graph_sizeph_size = '../launches/igor/johnstone2010.yaml', 12, 54\n",
    "\n",
    "data_conv = gen_episodes_progress(ignore_saved=True,\n",
    "    context='conveyors', num_episodes=10000, random_seed=42,\n",
    "    run_params=scenario, save_path='../logs/data_conveyor1_oneinp_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_conv.loc[:, 'working'] = 1.0\n",
    "data_conv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_emb = CachedEmbedding(LaplacianEigenmap, dim=emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conveyor_network_ng_emb = QNetwork(graph_size, scope='conveyor_test_ng', activation='relu',\n",
    "                                   layers=[64, 64], embedding_dim=conv_emb.dim)\n",
    "conveyor_network_ng_emb_ws = QNetwork(graph_size, scope='conveyor_test_ng', activation='relu',\n",
    "                                      layers=[64, 64], additional_inputs=[{'tag': 'working', 'dim': 1}],\n",
    "                                      embedding_dim=conv_emb.dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conveyor_network_ng_emb_losses = qnetwork_pretrain(conveyor_network_ng_emb, shuffle(data_conv), epochs=10,\n",
    "                                                   embedding=conv_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conveyor_network_ng_emb_ws_losses = qnetwork_pretrain(conveyor_network_ng_emb_ws, shuffle(data_conv), epochs=20,\n",
    "                                                      embedding=conv_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses({\n",
    "    'no_inp': conveyor_network_ng_emb_losses,\n",
    "    'work_status': conveyor_network_ng_emb_ws_losses,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}